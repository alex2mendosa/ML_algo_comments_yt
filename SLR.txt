library(ggplot2)
library(dplyr)

1.1 - What is Simple Linear Regression?
  SLR us statistical method that allows us to summarize and study 
relationships between two continuous (quantitative) variables
One variable is denoted as x , referred as independent variable or
predictor or featyre.
Another variable is referred as y or dependent variable or 
response or outcome.
By relations is implied that changes in x cause changes in 
y.
we also talk about statistical ralationshisps which imply that
changes in x due to changes in y may contain diviation depending 
on sample of data in x.
list.files("Reg_Datasets")
d1<-mtcars[,c(1,2,3,4)]

ggplot(data=d1,aes(hp,mpg)) + geom_point(col="red") + 
  geom_smooth(method="lm")
Therefore, as mpg increases, we expect hp to increase, as more powerfull
cars requre more gasoline to run.
As our relation as statistical, plot demonstates
scatter, indicaring that line does not 
go through all points.


#1.2 - What is the "Best Fitting Line"?
We are intrested in average response of y given value of x.
Best fitted line has a form of
cat( "y=b0+b1x" )
lets create model
m1<-lm(hp~mpg,data=d1)
summary(m1)
coefficients(m1)

lets have a look what ig we plot values on x into formula 
and compare actual data versus fitted

d1<-mtcars[,c(1,2,3,4)]
d1<-d1 %>%  mutate(fit=coefficients(m1)[1]+coefficients(m1)[2]*mpg  )
ggplot(data=d1,aes(y=hp,x=mpg)) + geom_point(col="red") + 
  geom_line(aes(y=fit),col="blue",cex=1 )

All values producesby equation form straight line
If we want to know value of hp given x which isnot in our dataset,
we can still use equation to predict value of y

There would be error between forecated values and actual,
let denote it as er

d1<-d1 %>%  mutate(er=hp-fit  )
We want to find coefficients of betta whcih would mimnimise sum of errors
In this case, we wil call for leas square criterion 
which minimise the sum of the squared fitted errors.

The equation of line is 
cat( "y=b0+b1x" )
we need to find b0 and b1 that make the sum of squared errors
lowest as possible.
Therefore, we need tofind coefficients that
minimise 
cat( "sum( (y-y_hat)^2 )"  )
If we dont use square prediction error would cancel each other.
Therefore, we need to use calculus to find max of function
cat( "sum( (y- (b0+b1x)  )^2 )"  )
We need to differentioate it with respect to b1 and find it.

If we do it , leat squares estimates are:
  cat( "b0=mean(y)-b1(mean(x))" )

cat( "  sum[ (x-mean)(y-mean(y)) ] / sum( (x-mean(x)^2 )   " )
or 
cat(  "Cov(x,y)/var(x)"  )

Line which results from the respective coefficients is 
known as least squares line or least squares regression line

The respective approach is appropriate for data which follows
a linear trend.
Sign of betts is determined by numerator, which is the
prduct of 2 distances.

we can print summary of regresision via
summary(m1) 
which allows to implement standard regression analysis
sum(d1$er^2)



with regresisoin line we can predict values which are not
in our dataset, ut we need to base our forecast
on x which is inside scope of model and our 
dataset cover range of data used for prediction. 

b1  tells us that if we increase x by 1, 
y would increase by b1.
In general, we can expect the mean response to increase or
decrease by b1 units for every one unit increase in x.


#1.3 - The Simple Linear Regression Model

d2<-ToothGrowth

ggplot(data=d2,aes(x=dose,y=len) )+ geom_point(col="red")
names(d2)

Lets estimate mean  of each group
d2 %>% group_by(dose) %>% summarise(mean1=(mean(len)))

how lets estimate regression
m2<-lm(len~dose,data=d2)
and have a look on moel response if we provide dosage as input

predict(m2,newdata = data.frame( dose=c(0.5,1,2) ))
Values are differetn as we are converned about decreasing 
sum of errors across all samples.

In estimating regression, we have to rely on sample to estimate
population regression line.Therefore, we assume 
existance of true coefficients but able to estimate
only sample coeffictients

lest sample our data nad resstimte coefficients
d2.2<-slice_sample(d2,prop=0.7)
m2.2<-lm(len~dose,data=d2.2)
coefficients(m2)
coefficients(m2.2)

Values are close but not identical , lets decrease sample
d2.2<-slice_sample(d2,prop=0.5)
m2.2<-lm(len~dose,data=d2.2)
coefficients(m2)
coefficients(m2.2)
Now difference is more apparent.

lets collect 100 balues of b1
col_b1<-as.vector(rep(0,100),mode="numeric")

f1<-function(x){
  x1<-slice_sample(x,prop=0.5)
  mf.2<-lm(len~dose,data=x1)
  out1<-coefficients(mf.2)[2]
  return(out1)
}

f1(d2)

col_b1<-replicate( 10000,f1(d2) ) %>% unname()
hist(col_b1,breaks = 40)

We can see that distibution of betts is normal and approaches 
true mean of betta
coefficients(m2)[2]
mean(col_b1)

The follwoong assumption comprise
engine of simple linear  regression 
Mean ressponse of y is linear fucntion of x
Linear relationship

Errors are independent
No auto-correlation

Errors are normally distibuted

Errirs at each predictor have equal variance.
Homoscedasticity

ggplot(data=d2,aes(x=dose,y=len) )+ geom_point(col="red")
names(d2)

Norice we cant draw straing line which connect mean
poins of data if there are more that 2 groups of observations.


#1.4 - What is The Common Error Variance?

d2<-ToothGrowth

ggplot(data=d2,aes(x=dose,y=len) )+ geom_point(col="red")
names(d2)


lets check variance of each group
d2 %>% group_by(dose) %>%
  summarise(var(len))
var(d2$len)

Only varoance of group 2 is
different.
Variance of data , for whole population 
quantifies how much response of y vary arounf
the unknown mean, notice that 
variance og grouped data is lower
that overall variance.

Greater is variance of data, moe challenging is
forecasting  task.
Clustering as part of machine learning
includes variance minimisation to 
separate data in homogenous clusters.


To get an idea, therefore, of how 
precise future predictions would be, we need to know how much the responses (y) vary around the (unknown) 
mean population regression line

As always we dont know true variance 
of population, the best we can is to 
estimate sample variance.


cat("sample variance= sum( (y-mean(y))^2 )/(n-1) ")

As we use mean of population we use n-1
as it costs is 1 degree of freedom 
m2<-lm(len~dose,data=d2)
summary(m2)
var(d2$len)
sd(d2$len)

Lets apply same formula for errors
var( residuals(m2) )
sd( residuals(m2) )
This is residuals standard error
Manually
p1<-sum(residuals(m2)^2)
sqrt( p1/nrow(d2) )
sqrt( p1/(nrow(d2)-2) )
Therefore we need to divide it by n-2

This is residual standard error or regression 
standard error

#1.5 - The Coefficient of Determination, 

First we need to estimate 3 statistics
RSS<-sum( (fitted(m2)-mean(d2$len))^2  )
This are sum of distances of fitted values 
away from mean

ESS<-sum( (d2$len-fitted(m2))^2 )
This are sum of distances from actial data from 
fitted values

TSS<-sum( (d2$len-mean(d2$len))^2 )
This quantifies sum of distances of data 
around mean
summary(m2)
RSS/TSS

cat("TSS=RSS+ESS")
Therefore , formula explains 
variatiomn of data in terms of random variations and
variations explained by fitted model

cat("Multiple R-squared:RSS/TSS")
If the value if high it means regression 
explans large part of varioation in data.

definition is that r shos variation in y 
explained by variations in x, large value
is better.
Or
The % of vatriations reduced by taking into account 
predictor x


1.6 - (Pearson) Correlation Coefficient, 
m2<-lm(len~dose,data=d2)
summary(m2)
Correlation between data
can show information if there 
is comovement between variables
with(d2,cor(len,dose))
with(d2,cov(len,dose))
sqrt( with(d2,cor(len,dose)) )
coefficients(m2)

summary(m2)$r.squared
with(d2,cor(len,dose))

sqrt(summary(m2)$r.squared)
with(d2,cor(len,dose))
Therefore, for SLR, correlation 
between
x and y eqauls square root of rSquared

generraly correlation equals
cat(  " cov(x,y)/( sd(x)*sd(y) )  )"
      with(d2,  cov(len,dose)/( sd(dose)*sd(len) )   )
      
      Pearson’s correlation coefficient is 
      the test statistics that measures the 
      statistical relationship, or association, 
      between two continuous variables.  
      It is known as the best method of measuring
      the association between variables of interest because
      it is based on the method of covariance.
      
      oNe strong assumption must hold
      Linear relationship: Two variables 
      should be linearly related to each other. 
      This can be assessed with a scatterplot: 
        plot the value of variables on a scatter diagram, 
      and check if the plot yields
      a relatively straight line.
      
      
      #1.8 -  Cautions
      
      A large  value does not imply that the estimated 
      regression line fits the data well.
      
      list.files()
      d3<-read.csv("carstopping.csv") %>% as_tibble()
      ggplot(data=d3,aes(x=Speed,y=StopDist)) + 
        geom_point(col="red")+geom_smooth(method="lm")
      Plot shows positive stong association between
      variable with upward trend
      
      m<-lm(StopDist~Speed,data=d3)
      summary(m)$r.squared
      A large percent of variation can be explained 
      via Dit value
      
      p1<-with(d3, sum((StopDist-mean(StopDist))^2)  )
      p2<-with(d3, sum((fitted(m)-mean(StopDist))^2)  )
      p2/p1
      
      
      #2
      file.create("mccoo.csv")
      d3<-read.csv("mccoo.csv") %>% as_tibble()
      ggplot(data=d3,aes(x=McCoo,y=Score)) + 
        geom_point(col="red")+geom_smooth(method="lm")
      m<-lm(McCoo~Score,data=d3)
      summary(m)$r.squared
      
      m<-lm(McCoo~Score,data=d3)
      summary(m)$r.square
      
      d33<-filter(d3,McCoo!=206)
      m<-lm(McCoo~Score,data=d33)
      summary(m)$r.squared
      
      
      #3
      We shoul dkeep in mind that
      The predictor x does indeed cause the
      changes in the response y.
      
      The causal relation may instead be reversed. That is, the response y 
      may cause the changes in the predictor x.
      
      The predictor x is a contributing but not sole cause of 
      changes in the response variable y.
      
      There may be a "lurking variable" 
      is the real cause of changes in y 
      but also is associated with x, 
      thus giving rise to the observed 
      relationship between x and y.
      
      The association may be purely coincidental.
      
      #3
      d3<-read.csv("drugdea.csv") %>% as_tibble()
      names(d3)
      
      ggplot(data=d3,aes(x=budget,y=deaths))+  
        geom_point(col="red")+
        geom_smooth(method="lm")
      summary( lm(deaths~budget,data=d3))$r.squared
      
      ggplot(data=d3,aes(y=budget,x=deaths))+  
        geom_point(col="red")+
        geom_smooth(method="lm")
      summary( lm(budget~deaths,data=d3))$r.squared
      
      ggplot(data=d3,aes(y=deaths,x=year))+  
        geom_point(col="red")+
        geom_smooth(method="lm")
      summary( lm(deaths~year,data=d3))$r.squared
      
      
      #4
      
      d3<-read.csv("infant.csv") %>% as_tibble()
      names(d3)
      
      ggplot(data=d3,aes(x=feeding,y=death))+  
        geom_point(col="red")+
        geom_smooth(method="lm")
      
      summary( lm(death~feeding,data=d3))$r.squared
      
      ggplot(data=d3,aes(x=water,y=death))+  
        geom_point(col="red")+
        geom_smooth(method="lm")
      
      summary( lm(death~water,data=d3))$r.squared
      
      Water is a lurking variable here and is likely the 
      real driver behind infant death rates.
      
      
      #5
      
      d3<-read.csv("practical.csv") %>% as_tibble()
      names(d3)
      ggplot(data=d3,aes(x=x,y=y))+  
        geom_point(col="red")+
        geom_smooth(method="lm")
      
      m<-lm(y~x,data=d3)
      summary(m)
      with(d3,cor(x,y) )
      
      The large sample size results in a sample 
      slope that is significantly different 
      from 0, but not meaningfully different
      from 0. The scatterplot, which should 
      always accompany a simple linear 
      regression analysis, illustrates.
      
      b1<-coefficients(m)[2]
      c(b1-2*0.005576,b1+2*0.005576   )
      
      lets collect 100 balues of b1
      col_b1<-as.vector(rep(0,100),mode="numeric")
      
      f1<-function(a){
        x1<-slice_sample(a,prop=0.5)
        mf.2<-lm(y~x,data=x1)
        out1<-coefficients(mf.2)[2]
        return(out1)
      }
      
      f1(d3)
      
      col_b1<-replicate( 1000,f1(d3) ) %>% unname()
      c(b1-2*0.005576,b1+2*0.005576   )
      # here we estimate conf limits with simulation
      
      
      #6
      
      d3<-read.csv("oldfaithful.csv") %>% as_tibble()
      names(d3)
      ggplot(data=d3,aes(x=duration,y=next.))+  
        geom_point(col="red")+
        geom_smooth(method="lm")
      
      m<-lm(next.~duration,data=d3)
      summary(m)
      with(d3,cor(x,y) )
      
      
      
      #1.9 - Hypothesis Test for 
      #the Population Correlation Coefficient   
      
      d2<-ToothGrowth %>% as_tibble()
      
      r squared and corrlation  namely, 
      the two measures summarize the strength of a linear 
      relationship in samples only. 
      If we obtain different samples we obtain differet correlations
      values
      
      col_b1<-as.vector(rep(0,100),mode="numeric")
      
      f1<-function(x){
        x1<-slice_sample(x,prop=0.5)
        mf.2<-lm(len~dose,data=x1)
        out1<-coefficients(mf.2)[2]
        out2<-summary(mf.2)$r.squared
        return(out2)
      }
      
      f1(d2)
      
      col_b1<-replicate( 1000,f1(d2) ) %>% unname()
      summary(col_b1)
      
      here we have different values of r squared
      here is value for whole sample
      m2<-lm(len~dose,data=d2)
      summary(m2)$r.squared
      
      #therefore , we need to ficn a way o draw
      conclusion on confidence intrvals
      of regression statistics, as all 
      variables form a random variables
      originating from random samples
      
      # 2 ways to do to is to use 
      hypothesis testing or estimate
      confidence intervals
      
      #lets test is betta is signifficanntly
      dirrerent from 0, meaning it explaines
      variations in y
      
      First lets check under null hyphotesis 
      is there correlation between dose and length
      
      Forst we form hyphothesis
      
      Nest we estimate test statistics by
      calculatin t values
      
      Weuse t statistics to estimate p values, 
      it shouws probability of observe t statistics
      given that there is no relation between the respective values
      
      
      If the P-value is smaller
      than the significance level , 
      we reject the null hypothesis in favor of
      the alternative. We conclude "there is 
 sufficient evidence at the level to conclude 
 that there is a linear relationship in the
 population
 between the predictor x and response y."
      
      d3<-read.csv("Reg_Datasets//poverty.csv") %>%
        as_tibble()
      
      ggplot(data=d3,aes(PovPct,Brth15to17)) +
        geom_point(col="red")+
        geom_smooth(method="lm")
      
      m22<-lm(Brth15to17~PovPct,data=d3)
      summary(m22)
      remember , regression describes
      average, the most probable, response of
      y given x
      from r.squared, we see that poverty
      explains 53% of variation of y.
      
      
      d3<-read.csv("Reg_Datasets//lungs_cop.csv") %>%
        as_tibble()
      
      ggplot(data=d3,aes(Age,FEV)) +
        geom_point(col="red")+
        geom_smooth(method="lm")
      
      lm(FEV~Age,data=d3)
      
      Variace of observations 
      increases as age increases, soit is not constant
      Therefore variance is not constant.
      
      
      This illustrates that it is important 
      to be aware of how you are analyzing your data.
      If you only use a subset of your data that 
      spans a shorter range of predictor values,
      then you could obtain noticeably different results
      than if you had used the full dataset.
      
      
      
      #### Lesson 2: SLR Model Evaluation
      
      Geerally, 2 ways exist to check
      whether there exist linear association 
      between pedictor and response variabel
      
      One is t test for the slope and another is
      analysis of variance(ANOVA) F test.
      
      Therefore, 
      We could estimate the regression line 
      and then use the t-test to determine if the slope, ,
      of the population regression line is 0.
      
      Or 
      we could perform an (analysis of variance) F-test
      
      #2.1 - Inference 
      #for the Population Intercept and Slope
      
      We shoul dbe concerned about the whole population, 
      not the sample. Here confidence
      intervals and hypothesis tests can help us to
      learn about how far sample coefficients are away 
      from true paramaters.
      
      d2
      
      d2<-ToothGrowth %>% as_tibble()
      we can get coefficient but we we eant to know
      if there is relationships between the population
      of all the doses and lenght
      that is, we want to know if the population
      slope b1 is unlikely to be 0.
      
      confidence interval hs the formula
      cat(   "Residual standard error/ sum(x-mean(x)^2) "  )
      
      The resulting confidence interval not only
      gives us a range of values that is likely
      to contain the true unknown value . It also 
      allows us to answer the research question 
      "is the predictor x linearly related to the response y?" 
      If the confidence interval for  contains 0, then we 
      conclude that there is no evidence of a linear relationship 
      between the predictor x and the response y in the
      population. On the other hand, if the confidence 
      interval for does not contain 0, then we conclude 
      that there is evidence of a linear relationship 
      between the
      predictor x and the response y in the population.
      
      
      Anothe way is to use hypothesis testing via
      t score
      t score has te same concept as z score
      
      cat(  "(b1-b_true)/ se(b1)   " )
      #how many sd deviations b1 is way from mean 
      Third, we use the resulting test statistic 
      to calculate the P-value. As always, the
      P-value is the answer to the question how likely 
      is it that we’d get a test statistic t* as 
      extreme as we did if the null hypothesis were true?
        
        d1<-read.csv("Reg_Datasets//skin.csv") %>%
        as_tibble()
      
      m1<-lm(Mort~Lat,data=d1) 
      summary(m1) 
      
      This is how we estimate sd of betta
      mse<- sqrt( sum(residuals(m1)^2 )/47 )
      se<-mse/with(d1, sqrt(sum((Lat-mean(Lat))^2) ) )
      -5.9776/se # this is t value as we assume that true betta is 0
      
      #therefore, betta is to far from mean of 0
      #htat why we regect niull hyphothesis
      # we can eve use other value
      #tan betta
      
      #now back to confidence intervals
      
      c(-5.9776-2*se,-5.9776+2*se)
      There is 95% confidence that 
      true population slope 
      is beteween the respective values.
      
      # following factors affect width of confidence
      interval:
        Recall that, in general, we want 
      our confidence intervals to be as narrow 
      as possible. If we know what factors affect the 
      length of a confidence interval for the slope , 
      we can control
      them to ensure that we obtain a narrow interval. 
      
      formula is the following
      cat(  "t* (Res_St_Er/ sqrt(SSX) ) "   )      
      
      t as constant should reflect value for at least 90%
      As t decrease Ci also decreases
      notice , we can use confint.lm() to
      we estimate cond intervals for betta1
      
      As MSE decreases , CI wisth also decreases.
      The value of MSE depends on only two factors —
      how much the responses vary naturally around 
      the estimated regression line, and how well your
      regression function (line) fits the data.
      
      The more spreas out the predictor is 
      x-mean(x), which shows the spread of predictor,
      the larder is denominator,the smaller,
      narrower is the interval.
      Therefore , we can decrease interval ensuring that
      data is spread out.
      
      
      Moreoverm the grewater is the samle, larger is
      n-1 and hence width of CI becomes smaller.
      
      
      ## What if we cant rehect H0 and betta is not
      signifficatlt dirretect from 0:
        We committed a Type II error. That is,
      in reality  and our sample data just didnt 
      provide enough evidence to conclude that ≠ 0
      
      There really is not much of a linear
      relationship between x and y.
      
      There is a relationship between
      x and y — it is just not linear.
      
      Again, generally CI is estimated via:
        cat(" sample estimate +- t*SE ")
      
      d1<-read.csv("Reg_Datasets//leadcord.csv") %>%
        as_tibble()
      
      m1<-lm(Cord~Sold,data=d1) 
      summary(m1) 
      ggplot(data=d1,aes(Cord,Sold) )+
        geom_point(col="red")+
        geom_smooth(method="lm")
      
      confint(m1)
      
      
      #2.3 - Sums of Squares
      d1<-read.csv("Reg_Datasets//skin.csv") %>%
        as_tibble()
      
      m1<-lm(Mort~Lat,data=d1) 
      summary(m1) 
      ggplot(data=d1,aes(Lat,Mort) )+
        geom_point(col="red")+
        geom_smooth(method="lm")
      
      confint(m1)
      # let suse Ci and hyphothesis testing to
      check if there are linear relationsships between
      x and y, in other words betta is dofferent from 0
      
      mse<-sd( residuals(m1) )
      se<-mse/ sqrt(sum((d1$Lat-mean(d1$Lat))^2))
      t_stat<--5.9776/se
      
      There is alernttive way to check presence of 
      linear relationships, 
      speficically F test, analysis of variance.
      So fot t stat we havel p values
      We have p values for F stat as well.
      Both are random variables
      
      Now lest fo back to
      RSS, ESS TSS
      
      ggplot(data=d1,aes(Lat,Mort) )+
        geom_point(col="red")+
        geom_smooth(method="lm")+
        geom_hline(yintercept = mean(d1$Mort),col="green" )
      
      TSS<-with(d1, sum((Mort-mean(Mort))^2) )
      this is distance between no relations value
      agains observations
      
      ESS<-with(d1, sum((Mort-fitted(m1))^2) )      
      The distance of each observed value 
      from the estimated regression line       
      
      RSS<-with(d1, sum((fitted(m1)-mean(Mort))^2) )     
      The distance of each fitted value  from the 
      no regression line  is       
      
      Therefore diviations from mean are explained by 2 
      components. If RSS is large enouthd, 
      model fits data well, demonstating it
      thtough r squared values
      summary(m1)$r.squared
      
      
      #2.4 - Sums of Squares (continued)
      
      Now agains , can we check presence of
      linear relations using not means 
      but varinca
      
      d1<-read.csv("Reg_Datasets//hgpa.csv") %>%
        as_tibble()
      
      m1<-lm(height~gpa,data=d1) 
      summary(m1) 
      ggplot(data=d1,aes(height,gpa) )+
        geom_point(col="red")+
        geom_smooth(method="lm")
      
      confint(m1)
      Here coninf includes 0
      summary(m1)
      and betta is not sognifficantly different 
      from 0
      here is not enough statistical evidence to 
      conclude that the slope is not 0.
      We conclude that there is no linear relationship
      between height and grade point average.
      
      Following R squaredm we conclude that 
      vary liitle if the vatriations can be attributed to
      height.
      
      #2.5 - Analysis of Variance: The Basic Idea
      
      TSS can be broken into
      RSS and ESS.
      If the regression sum of squares is a "large" 
      component of the total sum of squares, it suggests that there is a linear association between 
      the predictor x and the response y.
      
      
      #2.6 - The Analysis 
      of Variance (ANOVA) table and the F-test
      
      d1<-read.csv("Reg_Datasets//skin.csv") %>%
        as_tibble()
      
      m1<-lm(Mort~Lat,data=d1) 
      summary(m1) 
      ggplot(data=d1,aes(Lat,Mort) )+
        geom_point(col="red")+
        geom_smooth(method="lm")
      
      confint(m1)
      
      degree of fredom fot
      SSTO is just n-1,
      for ESS its n-2, for SSR is n-1
      
      cat("mse=  ESS/n-2  ") $ sd for regression errots
      cat("msr=sum((fitted-mean)^2)")
      
      
      one more time to start before F test and ANOVA
      
      cat( "mse=sse/n-1" ) # this is variacne of error term
      car("msr=ssr/1")# this ia ao,p;y sum of differences between
      fitted values and mean
      We dividew by n-1 bacause the repsctive values
      are dirived from sample.
      
      Therefore, we can use the respective measures to test null
      hyphothesis.
      
      Lets simulate different values for MSR
      
      
      col_b1<-as.vector(rep(0,100),mode="numeric")
      
      f1<-function(x){
        x1<-slice_sample(x,prop=0.5)
        mf.2<-lm(Mort~Lat,data=x1)
        out1<- sum( ( fitted(mf.2)-mean(x1$Mort)  )^2 )
        # the respective value is not part of sratistics
        # and it is not residual standard error
        return(out1)
      }
      
      f1(d1)
      
      col_b1<-replicate( 1000,f1(d1) ) %>% unname()
      hist(col_b1,breaks = 40)
      mean(col_b1)
      sqrt(mean(col_b1))
      
      if netts is 0 then ratio MSR/MSE shoul be 1
      
      F statistics is MSR/MSE or SSR/MSE for SLR
      therefore is is ration of 2 variance ans n-1 in cancelled
      
      The residual sum of squares measures ESS
      the amount of error remaining
      between the regression function and the data set.      
      
      p1<-sum( ( fitted(m1)-mean(d1$Mort)  )^2 )  
      mse<-sum(residuals(m1)^2)
      f<-p1/mse      
      
      As always, the P-value is obtained by answering the question:
        What is the probability that we’d get an F
      statistic as large as we did, 
      if the null hypothesis is true?   
        
        d1<-read.csv("Reg_Datasets//mens200m.csv") %>%
        as_tibble()
      
      m1<-lm(Men200m~Year,data=d1) 
      summary(m1) 
      ggplot(data=d1,aes(Year,Men200m) )+
        geom_point(col="red")+
        geom_smooth(method="lm")
      
      confint(m1)    
      rss<-sum( ( fitted(m1)-mean(d1$Men200m)  )^2 )    
      ess<-sum( ( d1$Men200m-fitted(m1)  )^2 )   
      sum( ( residuals(m1)  )^2 )   
      
      residua_error<-sqrt(ess)  
      
      
      # Equivalent linear relationship tests
      overall following methods are appropriate to test presence
      of linear relationships
      t test, ANOVA  and cond intervals
      
      One more, RSS  should be used to determine
      how much of the total error is due to lack of model
      fitm we determine how far the average 
      observed response at each x-value is from the predicted 
      response of each data point. 
      
      Therefore, fitted value minuas mean respose sed
      with TSS is used to determine lack of fit.
      
      To determine how much of the total error is due to just random error, we determine how far each observed response is from
      the average observed response at each x-value.
      this iS TSS
      
      #### Lesson 3: SLR Estimation & Prediction
      regeresion analysis involves
      model formulation
      model estimation
      model evaluation and 
      model use
      
      ,ean of y showns mean response withput
      taking into account x variable, therefore, regression
      shows conditiona mean of y.
      
      Con make a forecastmbecause we deal with
      samples, we need to estomate 
      CI for intervam mean and prediction 
      intervalf ro new value. Mean and single 
      observations therefore. 
      
      #Confidence Interval for the Mean Response
      Ci intevals is usually about estimatinf
      t value
      cat("sample estimate +- t * se of estimate")
      
#4.1 - Background
  
      # 4 conditon  should be applied sot he
# linear regression is meaningfull from inference point of view
      
Mean response if y is lineat function of x

Errors are independen and mormally distibuted

Errors at each value of predictor have
equla variance.

overall assumpton imply that
errors shoud be independent , indentically
distributed random varibale.

Therefore , the follwong errors may apper:
  
lerations are not linear,
and errores are nt IID

We shoukd also be able to identify outliers and
check if our regression misses importat
expanatory variable

All tests and intervals are very sensitive to 
even minor departures from independence as
they are calculated directly from residuals

The hypothesis tests and confidence intervals for  coef
and  are fairly "robust" 
(that is, forgiving) against departures from normality.

Prediction intervals are quite
sensitive to departures from normality.

  All starts with 
  residul.
  cat( "e=yi-yf" )

  When conducting a residual analysis, 
  a "residuals versus fits plot" 
  is the most frequently created plot. 

  The plot is used to detect non-linearity,
  unequal error variances, and outliers.
  
 we want that  And, it illustrates that 
 the variation around the estimated regression line is constant suggesting
 that the assumption of equal error variances is reasonable. 
  
 d1<-read.csv("Reg_Datasets//alcstren.csv") %>%
   as_tibble()
 
 m1<-lm(strength~alcohol,data=d1) 
 summary(m1) 
 ggplot(data=d1,aes(alcohol,strength) )+
   geom_point(col="red")+
   geom_smooth(method="lm")
 
 confint(m1)     
 
 plot(x=fitted(m1)  ,y=residuals(m1) )
  
 Value of 0 appear for the perfect fit. 
  
Why the plot works:
  
The residuals "bounce randomly" around the residual = 0 line.
This suggests that the 
assumption that the relationship is linear is reasonable.  

The residuals roughly form a "horizontal band" around the residual = 0 line. This suggests
that the variances of the error terms are equal.

No one residual "stands out" from the basic random pattern of residuals. 
This suggests that there are no outliers.
  
  
#4.3 - Residuals vs. Predictor Plot

the residuals vs. predictor plot offers no new
information to that which is 
already learned by the residuals vs. fits plot.
  
But it cal help to accees contribution of x values
candidates.
If we formed a regression , we can plot estomated residuals
agains candidate for x to find possible 
pattern between resudualt and unused candidate.

If a plot of the "new response" against 
a predictor shows a non-random pattern, it indicates 
that the predictor explains some of the 
remaining variability in the new (adjusted) response. 
Threfore, new predictor can explain remainig 
variability in data.

If new x candidate versus estimated resuduals
does not indicate pattern,no significant amount of
the remaining variability 
can be explained by the individuals' durations.



  
  
  
  


  
  

